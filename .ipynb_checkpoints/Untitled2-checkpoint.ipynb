{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa615f16-7c02-45e0-a2ab-726bfa87c757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Read the dataset\n",
    "diabetes = pd.read_csv(\"diabetes_dataset.csv\")\n",
    "\n",
    "# Extract information from the dataset\n",
    "num_features = diabetes.shape[1] - 1\n",
    "num_instances = diabetes.shape[0]\n",
    "missing_values = diabetes.isnull().sum().sum()\n",
    "\n",
    "# Outlier detection using Z-score method\n",
    "z_scores = np.abs((diabetes - diabetes.mean()) / diabetes.std())\n",
    "outliers = (z_scores > 3).sum().sum()\n",
    "\n",
    "# Data separation into X and Y\n",
    "y = diabetes[\"Outcome\"]\n",
    "x = diabetes.drop('Outcome', axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Building\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate model performance for Decision Tree\n",
    "dt_train_accuracy = accuracy_score(Y_train, dt_model.predict(X_train))\n",
    "dt_test_accuracy = accuracy_score(Y_test, dt_model.predict(X_test))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate model performance for Random Forest\n",
    "rf_train_accuracy = accuracy_score(Y_train, rf_model.predict(X_train))\n",
    "rf_test_accuracy = accuracy_score(Y_test, rf_model.predict(X_test))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate model performance for Logistic Regression\n",
    "lr_train_accuracy = accuracy_score(Y_train, lr_model.predict(X_train))\n",
    "lr_test_accuracy = accuracy_score(Y_test, lr_model.predict(X_test))\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate model performance for Support Vector Machine\n",
    "svm_train_accuracy = accuracy_score(Y_train, svm_model.predict(X_train))\n",
    "svm_test_accuracy = accuracy_score(Y_test, svm_model.predict(X_test))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate model performance for Naive Bayes\n",
    "nb_train_accuracy = accuracy_score(Y_train, nb_model.predict(X_train))\n",
    "nb_test_accuracy = accuracy_score(Y_test, nb_model.predict(X_test))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "if len(X_train) > 5:\n",
    "    knn_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate model performance for K-Nearest Neighbors\n",
    "    knn_train_accuracy = accuracy_score(Y_train, knn_model.predict(X_train))\n",
    "    knn_test_accuracy = accuracy_score(Y_test, knn_model.predict(X_test))\n",
    "else:\n",
    "    knn_train_accuracy = '-'\n",
    "    knn_test_accuracy = '-'\n",
    "    warnings.warn(\"K-Nearest Neighbors model requires at least 5 samples.\")\n",
    "\n",
    "# Create a DataFrame for model results\n",
    "model_results = pd.DataFrame([\n",
    "    ['Decision Tree', round(dt_train_accuracy, 2), round(dt_test_accuracy, 2)],\n",
    "    ['Random Forest', round(rf_train_accuracy, 2), round(rf_test_accuracy, 2)],\n",
    "    ['Logistic Regression', round(lr_train_accuracy, 2), round(lr_test_accuracy, 2)],\n",
    "    ['Support Vector Machine', round(svm_train_accuracy, 2), round(svm_test_accuracy, 2)],\n",
    "    ['Naive Bayes', round(nb_train_accuracy, 2), round(nb_test_accuracy, 2)],\n",
    "    ['K-Nearest Neighbors', knn_train_accuracy, knn_test_accuracy]\n",
    "], columns=['Method', 'Training Accuracy', 'Test Accuracy'])\n",
    "\n",
    "# Convert K-Nearest Neighbors accuracy scores to string format\n",
    "model_results['Training Accuracy'] = model_results['Training Accuracy'].astype(str)\n",
    "model_results['Test Accuracy'] = model_results['Test Accuracy'].astype(str)\n",
    "\n",
    "# Handle missing values for K-Nearest Neighbors accuracy scores\n",
    "model_results.loc[model_results['Method'] == 'K-Nearest Neighbors', 'Training Accuracy'] = knn_train_accuracy\n",
    "model_results.loc[model_results['Method'] == 'K-Nearest Neighbors', 'Test Accuracy'] = knn_test_accuracy\n",
    "\n",
    "# Print the model results\n",
    "print(model_results)\n",
    "\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Dataset ID: Dataset\")\n",
    "print(\"No. of Features:\", num_features)\n",
    "print(\"Number of Instances:\", num_instances)\n",
    "print(\"Missing Values:\", missing_values)\n",
    "print(\"Outliers:\", outliers)\n",
    "print()\n",
    "print(\"Accuracy Scores:\")\n",
    "print(model_results)\n",
    "print()\n",
    "print(\"Best Algorithm: Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c93f2eb3-8a42-4411-8c20-17c039d7fb13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: Dataset\n",
      "No. of Features: 9\n",
      "Number of Instances: 5\n",
      "Missing Values: 0\n",
      "Outliers: Not implemented\n",
      "Feature Correlations:\n",
      "                           Unnamed: 0  Pregnancies   Glucose  BloodPressure   \n",
      "Unnamed: 0                  1.000000    -0.532414 -0.068648      -0.813326  \\\n",
      "Pregnancies                -0.532414     1.000000  0.797993       0.509708   \n",
      "Glucose                    -0.068648     0.797993  1.000000      -0.104300   \n",
      "BloodPressure              -0.813326     0.509708 -0.104300       1.000000   \n",
      "SkinThickness              -0.065341    -0.620394 -0.489174      -0.225860   \n",
      "Insulin                     0.890225    -0.683764 -0.178347      -0.845910   \n",
      "BMI                         0.418270    -0.473810  0.030986      -0.738771   \n",
      "DiabetesPedigreeFunction    0.586717    -0.288684  0.334031      -0.926860   \n",
      "Age                        -0.665446     0.493862  0.483465       0.216797   \n",
      "Outcome                     0.000000     0.563547  0.911585      -0.322832   \n",
      "\n",
      "                          SkinThickness   Insulin       BMI   \n",
      "Unnamed: 0                    -0.065341  0.890225  0.418270  \\\n",
      "Pregnancies                   -0.620394 -0.683764 -0.473810   \n",
      "Glucose                       -0.489174 -0.178347  0.030986   \n",
      "BloodPressure                 -0.225860 -0.845910 -0.738771   \n",
      "SkinThickness                  1.000000  0.371826  0.727738   \n",
      "Insulin                        0.371826  1.000000  0.750166   \n",
      "BMI                            0.727738  0.750166  1.000000   \n",
      "DiabetesPedigreeFunction       0.323402  0.716031  0.852934   \n",
      "Age                            0.349494 -0.385997  0.295061   \n",
      "Outcome                       -0.100599  0.064545  0.422899   \n",
      "\n",
      "                          DiabetesPedigreeFunction       Age   Outcome  \n",
      "Unnamed: 0                                0.586717 -0.665446  0.000000  \n",
      "Pregnancies                              -0.288684  0.493862  0.563547  \n",
      "Glucose                                   0.334031  0.483465  0.911585  \n",
      "BloodPressure                            -0.926860  0.216797 -0.322832  \n",
      "SkinThickness                             0.323402  0.349494 -0.100599  \n",
      "Insulin                                   0.716031 -0.385997  0.064545  \n",
      "BMI                                       0.852934  0.295061  0.422899  \n",
      "DiabetesPedigreeFunction                  1.000000  0.159455  0.606669  \n",
      "Age                                       0.159455  1.000000  0.646147  \n",
      "Outcome                                   0.606669  0.646147  1.000000  \n",
      "\n",
      "Accuracy Scores:\n",
      "Decision Tree R2 Score (Training): 1.0\n",
      "Decision Tree R2 Score (Test): nan\n",
      "Random Forest R2 Score (Training): 0.7933333333333333\n",
      "Random Forest R2 Score (Test): nan\n",
      "Logistic Regression Accuracy (Training): 1.0\n",
      "Logistic Regression Accuracy (Test): 1.0\n",
      "Support Vector Machine Accuracy (Training): 0.75\n",
      "Support Vector Machine Accuracy (Test): 0.0\n",
      "Naive Bayes Accuracy (Training): 1.0\n",
      "Naive Bayes Accuracy (Test): 0.0\n",
      "K-Nearest Neighbors Accuracy (Training): -\n",
      "K-Nearest Neighbors Accuracy (Test): -\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "/tmp/ipykernel_4529/319609246.py:110: UserWarning: K-Nearest Neighbors model requires at least 5 samples.\n",
      "  warnings.warn(\"K-Nearest Neighbors model requires at least 5 samples.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Read the dataset\n",
    "diabetes = pd.read_csv(\"diabetes_dataset.csv\")\n",
    "\n",
    "# Extract information from the dataset\n",
    "num_features = diabetes.shape[1] - 1\n",
    "num_instances = diabetes.shape[0]\n",
    "missing_values = diabetes.isnull().sum().sum()\n",
    "outliers = \"Not implemented\"  # You can add outlier detection logic here\n",
    "feature_correlations = diabetes.corr()\n",
    "\n",
    "# Data Separation into X and Y\n",
    "y = diabetes[\"Outcome\"]\n",
    "x = diabetes.drop('Outcome', axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Building\n",
    "# Decision Tree\n",
    "# Training the model\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train, Y_train)\n",
    "\n",
    "# Checking the performance of the model on the training set\n",
    "y_dt_train_pred = dt_model.predict(X_train)\n",
    "y_dt_test_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance for Decision Tree\n",
    "dt_train_mse = mean_squared_error(Y_train, y_dt_train_pred)\n",
    "dt_train_r2 = r2_score(Y_train, y_dt_train_pred)\n",
    "dt_test_mse = mean_squared_error(Y_test, y_dt_test_pred)\n",
    "dt_test_r2 = r2_score(Y_test, y_dt_test_pred)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(max_depth=2, random_state=100)\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Applying the model to make predictions\n",
    "y_rf_train_pred = rf_model.predict(X_train)\n",
    "y_rf_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance for Random Forest\n",
    "rf_train_mse = mean_squared_error(Y_train, y_rf_train_pred)\n",
    "rf_train_r2 = r2_score(Y_train, y_rf_train_pred)\n",
    "rf_test_mse = mean_squared_error(Y_test, y_rf_test_pred)\n",
    "rf_test_r2 = r2_score(Y_test, y_rf_test_pred)\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Applying the model to make predictions\n",
    "y_lr_train_pred = lr_model.predict(X_train)\n",
    "y_lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance for Logistic Regression\n",
    "lr_train_accuracy = accuracy_score(Y_train, y_lr_train_pred)\n",
    "lr_test_accuracy = accuracy_score(Y_test, y_lr_test_pred)\n",
    "\n",
    "# Support Vector Machine Model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# Applying the model to make predictions\n",
    "y_svm_train_pred = svm_model.predict(X_train)\n",
    "y_svm_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance for Support Vector Machine\n",
    "svm_train_accuracy = accuracy_score(Y_train, y_svm_train_pred)\n",
    "svm_test_accuracy = accuracy_score(Y_test, y_svm_test_pred)\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Applying the model to make predictions\n",
    "y_nb_train_pred = nb_model.predict(X_train)\n",
    "y_nb_test_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance for Naive Bayes\n",
    "nb_train_accuracy = accuracy_score(Y_train, y_nb_train_pred)\n",
    "nb_test_accuracy = accuracy_score(Y_test, y_nb_test_pred)\n",
    "\n",
    "# K-Nearest Neighbors Model\n",
    "knn_model = KNeighborsClassifier()\n",
    "if len(X_train) > 5:\n",
    "    knn_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Applying the model to make predictions\n",
    "    y_knn_train_pred = knn_model.predict(X_train)\n",
    "    y_knn_test_pred = knn_model.predict(X_test)\n",
    "\n",
    "    # Evaluate model performance for K-Nearest Neighbors\n",
    "    knn_train_accuracy = accuracy_score(Y_train, y_knn_train_pred)\n",
    "    knn_test_accuracy = accuracy_score(Y_test, y_knn_test_pred)\n",
    "else:\n",
    "    knn_train_accuracy = '-'\n",
    "    knn_test_accuracy = '-'\n",
    "    warnings.warn(\"K-Nearest Neighbors model requires at least 5 samples.\")\n",
    "\n",
    "# Create a DataFrame for model results\n",
    "model_results = pd.DataFrame([\n",
    "    ['Decision Tree', dt_train_mse, dt_train_r2, dt_test_mse, dt_test_r2],\n",
    "    ['Random Forest', rf_train_mse, rf_train_r2, rf_test_mse, rf_test_r2],\n",
    "    ['Logistic Regression', '-', '-', '-', '-'],\n",
    "    ['Support Vector Machine', '-', '-', '-', '-'],\n",
    "    ['Naive Bayes', '-', '-', '-', '-'],\n",
    "    ['K-Nearest Neighbors', '-', '-', '-', '-']\n",
    "], columns=['Method', 'Training MSE', 'Training R2', 'Test MSE', 'Test R2'])\n",
    "\n",
    "# Update the accuracy scores in the DataFrame\n",
    "model_results.loc[model_results['Method'] == 'Logistic Regression', 'Training MSE'] = f\"{lr_train_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'Logistic Regression', 'Test MSE'] = f\"{lr_test_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'Support Vector Machine', 'Training MSE'] = f\"{svm_train_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'Support Vector Machine', 'Test MSE'] = f\"{svm_test_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'Naive Bayes', 'Training MSE'] = f\"{nb_train_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'Naive Bayes', 'Test MSE'] = f\"{nb_test_accuracy:.4f}\"\n",
    "model_results.loc[model_results['Method'] == 'K-Nearest Neighbors', 'Training MSE'] = knn_train_accuracy\n",
    "model_results.loc[model_results['Method'] == 'K-Nearest Neighbors', 'Test MSE'] = knn_test_accuracy\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Dataset ID: Dataset\")\n",
    "print(\"No. of Features:\", num_features)\n",
    "print(\"Number of Instances:\", num_instances)\n",
    "print(\"Missing Values:\", missing_values)\n",
    "print(\"Outliers:\", outliers)\n",
    "print(\"Feature Correlations:\\n\", feature_correlations)\n",
    "print()\n",
    "print(\"Accuracy Scores:\")\n",
    "print(\"Decision Tree R2 Score (Training):\", dt_train_r2)\n",
    "print(\"Decision Tree R2 Score (Test):\", dt_test_r2)\n",
    "print(\"Random Forest R2 Score (Training):\", rf_train_r2)\n",
    "print(\"Random Forest R2 Score (Test):\", rf_test_r2)\n",
    "print(\"Logistic Regression Accuracy (Training):\", lr_train_accuracy)\n",
    "print(\"Logistic Regression Accuracy (Test):\", lr_test_accuracy)\n",
    "print(\"Support Vector Machine Accuracy (Training):\", svm_train_accuracy)\n",
    "print(\"Support Vector Machine Accuracy (Test):\", svm_test_accuracy)\n",
    "print(\"Naive Bayes Accuracy (Training):\", nb_train_accuracy)\n",
    "print(\"Naive Bayes Accuracy (Test):\", nb_test_accuracy)\n",
    "print(\"K-Nearest Neighbors Accuracy (Training):\", knn_train_accuracy)\n",
    "print(\"K-Nearest Neighbors Accuracy (Test):\", knn_test_accuracy)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7233254c-3e0a-44de-9dd6-909386a43ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     33\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 34\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m     36\u001b[0m     results[clf_name] \u001b[38;5;241m=\u001b[39m accuracy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:234\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/neighbors/_base.py:810\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    808\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    815\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    816\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 5"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes_dataset.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate different classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[clf_name] = accuracy\n",
    "\n",
    "# Display the accuracy scores\n",
    "for clf_name, accuracy in results.items():\n",
    "    print(f'{clf_name}: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0ba87-e664-4c3d-9346-e1ac1938e3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
